{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXQYNiq6AevZ"
      },
      "source": [
        "# HSE 2022: Mathematical Methods for Data Analysis\n",
        "\n",
        "## Homework 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-26T16:48:20.566549Z",
          "start_time": "2020-09-26T16:48:19.893995Z"
        },
        "id": "EGAy5_I-Aevc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "from sklearn import datasets\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.regression.linear_model import OLSResults\n",
        "from statsmodels.regression.linear_model import RegressionResultsWrapper\n",
        "from math import sqrt\n",
        "import random\n",
        "import sys\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "sns.set(style=\"darkgrid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ullEuZvKAevf"
      },
      "source": [
        "### Data\n",
        "\n",
        "For this homework we use Dataset from seaborn on diamonds prices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "LkdUSR2WAevg"
      },
      "outputs": [],
      "source": [
        "data = sns.load_dataset('diamonds')\n",
        "\n",
        "y = data.price\n",
        "X = data.drop(['price'], axis=1)\n",
        "columns = data.drop(['price'], axis=1).columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIL5wEabAevi"
      },
      "source": [
        "## Linear regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTTbz-poAevi"
      },
      "source": [
        "#### 0. [0.25 points] Encode categorical variables.\n",
        "\n",
        "I used this description https://www.kaggle.com/datasets/shivam2503/diamonds for understanding significance of every variavle.\n",
        "\n",
        "cut: quality of the cut (Fair, Good, Very Good, Premium, Ideal)\n",
        "\n",
        "color: from J (worst) to D (best)\n",
        "\n",
        "clarity: (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "HKBSQa2CAevj",
        "outputId": "eb02f516-864b-4fb1-abba-851722475af9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      carat        cut color clarity  depth  table  price     x     y     z\n",
              "0      0.23      Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n",
              "1      0.21    Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31\n",
              "2      0.23       Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31\n",
              "3      0.29    Premium     I     VS2   62.4   58.0    334  4.20  4.23  2.63\n",
              "4      0.31       Good     J     SI2   63.3   58.0    335  4.34  4.35  2.75\n",
              "...     ...        ...   ...     ...    ...    ...    ...   ...   ...   ...\n",
              "4995   0.70      Ideal     F    VVS2   61.9   54.8   3741  5.68  5.72  3.53\n",
              "4996   0.58      Ideal     D    VVS1   62.2   56.0   3741  5.34  5.36  3.33\n",
              "4997   0.90       Good     I    VVS1   63.9   63.0   3741  6.04  6.07  3.87\n",
              "4998   1.08       Good     J     SI2   63.2   59.0   3742  6.40  6.57  4.10\n",
              "4999   1.05  Very Good     I     SI2   62.3   59.0   3742  6.42  6.46  4.01\n",
              "\n",
              "[5000 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fe44b511-f489-4451-a23c-55751b9dacd7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>carat</th>\n",
              "      <th>cut</th>\n",
              "      <th>color</th>\n",
              "      <th>clarity</th>\n",
              "      <th>depth</th>\n",
              "      <th>table</th>\n",
              "      <th>price</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>z</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.23</td>\n",
              "      <td>Ideal</td>\n",
              "      <td>E</td>\n",
              "      <td>SI2</td>\n",
              "      <td>61.5</td>\n",
              "      <td>55.0</td>\n",
              "      <td>326</td>\n",
              "      <td>3.95</td>\n",
              "      <td>3.98</td>\n",
              "      <td>2.43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.21</td>\n",
              "      <td>Premium</td>\n",
              "      <td>E</td>\n",
              "      <td>SI1</td>\n",
              "      <td>59.8</td>\n",
              "      <td>61.0</td>\n",
              "      <td>326</td>\n",
              "      <td>3.89</td>\n",
              "      <td>3.84</td>\n",
              "      <td>2.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.23</td>\n",
              "      <td>Good</td>\n",
              "      <td>E</td>\n",
              "      <td>VS1</td>\n",
              "      <td>56.9</td>\n",
              "      <td>65.0</td>\n",
              "      <td>327</td>\n",
              "      <td>4.05</td>\n",
              "      <td>4.07</td>\n",
              "      <td>2.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.29</td>\n",
              "      <td>Premium</td>\n",
              "      <td>I</td>\n",
              "      <td>VS2</td>\n",
              "      <td>62.4</td>\n",
              "      <td>58.0</td>\n",
              "      <td>334</td>\n",
              "      <td>4.20</td>\n",
              "      <td>4.23</td>\n",
              "      <td>2.63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.31</td>\n",
              "      <td>Good</td>\n",
              "      <td>J</td>\n",
              "      <td>SI2</td>\n",
              "      <td>63.3</td>\n",
              "      <td>58.0</td>\n",
              "      <td>335</td>\n",
              "      <td>4.34</td>\n",
              "      <td>4.35</td>\n",
              "      <td>2.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4995</th>\n",
              "      <td>0.70</td>\n",
              "      <td>Ideal</td>\n",
              "      <td>F</td>\n",
              "      <td>VVS2</td>\n",
              "      <td>61.9</td>\n",
              "      <td>54.8</td>\n",
              "      <td>3741</td>\n",
              "      <td>5.68</td>\n",
              "      <td>5.72</td>\n",
              "      <td>3.53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4996</th>\n",
              "      <td>0.58</td>\n",
              "      <td>Ideal</td>\n",
              "      <td>D</td>\n",
              "      <td>VVS1</td>\n",
              "      <td>62.2</td>\n",
              "      <td>56.0</td>\n",
              "      <td>3741</td>\n",
              "      <td>5.34</td>\n",
              "      <td>5.36</td>\n",
              "      <td>3.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4997</th>\n",
              "      <td>0.90</td>\n",
              "      <td>Good</td>\n",
              "      <td>I</td>\n",
              "      <td>VVS1</td>\n",
              "      <td>63.9</td>\n",
              "      <td>63.0</td>\n",
              "      <td>3741</td>\n",
              "      <td>6.04</td>\n",
              "      <td>6.07</td>\n",
              "      <td>3.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4998</th>\n",
              "      <td>1.08</td>\n",
              "      <td>Good</td>\n",
              "      <td>J</td>\n",
              "      <td>SI2</td>\n",
              "      <td>63.2</td>\n",
              "      <td>59.0</td>\n",
              "      <td>3742</td>\n",
              "      <td>6.40</td>\n",
              "      <td>6.57</td>\n",
              "      <td>4.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4999</th>\n",
              "      <td>1.05</td>\n",
              "      <td>Very Good</td>\n",
              "      <td>I</td>\n",
              "      <td>SI2</td>\n",
              "      <td>62.3</td>\n",
              "      <td>59.0</td>\n",
              "      <td>3742</td>\n",
              "      <td>6.42</td>\n",
              "      <td>6.46</td>\n",
              "      <td>4.01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5000 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fe44b511-f489-4451-a23c-55751b9dacd7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fe44b511-f489-4451-a23c-55751b9dacd7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fe44b511-f489-4451-a23c-55751b9dacd7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ],
      "source": [
        "data.head(5000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.replace({'cut': {'Fair': 1, 'Good': 2, 'Very Good': 3, 'Premium': 4, 'Ideal': 5}},inplace=True)\n",
        "data.replace({'color': {'J': 1, 'I': 2, 'H': 3, 'G': 4, 'F': 5, 'E': 6, 'D': 7}}, inplace=True)\n",
        "data.replace({'clarity': {'I1': 1, 'SI2': 2, 'SI1': 3, 'VS2': 4, 'VS1': 5, 'VVS2': 6, 'VVS1': 7, 'IF': 8}}, inplace=True)\n",
        "X = data.drop(['price'], axis=1)\n",
        "data.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Cobzx4QOIOiM",
        "outputId": "cd118f74-a2f4-4d47-851d-9981b0f49a6a"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   carat  cut  color  clarity  depth  table  price     x     y     z\n",
              "0   0.23    5      6        2   61.5   55.0    326  3.95  3.98  2.43\n",
              "1   0.21    4      6        3   59.8   61.0    326  3.89  3.84  2.31\n",
              "2   0.23    2      6        5   56.9   65.0    327  4.05  4.07  2.31\n",
              "3   0.29    4      2        4   62.4   58.0    334  4.20  4.23  2.63\n",
              "4   0.31    2      1        2   63.3   58.0    335  4.34  4.35  2.75"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-11a4c0b7-3544-4f87-8c16-8c9d17f5ecec\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>carat</th>\n",
              "      <th>cut</th>\n",
              "      <th>color</th>\n",
              "      <th>clarity</th>\n",
              "      <th>depth</th>\n",
              "      <th>table</th>\n",
              "      <th>price</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>z</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.23</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>61.5</td>\n",
              "      <td>55.0</td>\n",
              "      <td>326</td>\n",
              "      <td>3.95</td>\n",
              "      <td>3.98</td>\n",
              "      <td>2.43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.21</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>59.8</td>\n",
              "      <td>61.0</td>\n",
              "      <td>326</td>\n",
              "      <td>3.89</td>\n",
              "      <td>3.84</td>\n",
              "      <td>2.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.23</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>56.9</td>\n",
              "      <td>65.0</td>\n",
              "      <td>327</td>\n",
              "      <td>4.05</td>\n",
              "      <td>4.07</td>\n",
              "      <td>2.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.29</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>62.4</td>\n",
              "      <td>58.0</td>\n",
              "      <td>334</td>\n",
              "      <td>4.20</td>\n",
              "      <td>4.23</td>\n",
              "      <td>2.63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.31</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>63.3</td>\n",
              "      <td>58.0</td>\n",
              "      <td>335</td>\n",
              "      <td>4.34</td>\n",
              "      <td>4.35</td>\n",
              "      <td>2.75</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-11a4c0b7-3544-4f87-8c16-8c9d17f5ecec')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-11a4c0b7-3544-4f87-8c16-8c9d17f5ecec button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-11a4c0b7-3544-4f87-8c16-8c9d17f5ecec');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0szs5p-Aevl"
      },
      "source": [
        "#### 1. [0.25 points] Split the data into train and test sets with ratio 80:20 with random_state=17."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "rXhvRqAsAevm"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.80, test_size=0.20, random_state=17)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGd6KlB_Aevn"
      },
      "source": [
        "#### 2. [1 point] Train models on train data using StatsModels library and apply it to the test set; use $RMSE$ and $R^2$ as the quality measure.\n",
        "\n",
        "* [`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html);\n",
        "* [`Ridge`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) with $\\alpha = 0.01$;\n",
        "* [`Lasso`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) with $\\alpha = 0.01$\n",
        "* [`ElasticNet`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html) with $\\alpha = 0.01$, $l_{1}$_$ratio = 0.6$\n",
        "\n",
        "Don't forget to scale the data before training the models with StandardScaler!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWi4KnhZAevo",
        "outputId": "88ecb9a4-b9eb-4803-9f17-896ab2d6b5ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simple Linear Regression:\n",
            "Test RMSE = 1240.1860\n",
            "Test R^2 = 0.9050\n",
            "\n",
            "Model with l2 regularization:\n",
            "Test RMSE = 1250.8344\n",
            "Test R^2 = 0.9034\n",
            "\n",
            "Model with l1 regularization:\n",
            "Test RMSE = 1240.1860\n",
            "Test R^2 = 0.9050\n",
            "\n",
            "Model with ElasticNet reqularization:\n",
            "Test RMSE = 1240.1860\n",
            "Test R^2 = 0.9050\n",
            "\n"
          ]
        }
      ],
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Experimentally, it was found that the quality of the model is better with a constant.\n",
        "X_train_with_const = sm.add_constant(X_train_scaled)\n",
        "X_test_with_const = sm.add_constant(X_test_scaled)\n",
        "\n",
        "# Simple Linear Regression.\n",
        "linear_regression = sm.OLS(y_train, X_train_with_const)\n",
        "simple_linear_results = linear_regression.fit()\n",
        "y_pred = simple_linear_results.predict(X_test_with_const)\n",
        "print(\"Simple Linear Regression:\")\n",
        "print(\"Test RMSE = %.4f\" % np.sqrt(mean_squared_error(y_test, y_pred)))\n",
        "print(\"Test R^2 = %.4f\\n\" % r2_score(y_test, y_pred))\n",
        "\n",
        "# Model with l2 regularization.\n",
        "ridge_regression = sm.OLS(y_train, X_train_with_const)\n",
        "ridge_results = ridge_regression.fit_regularized(alpha=0.01, L1_wt=0)\n",
        "y_pred = ridge_results.predict(X_test_with_const)\n",
        "print(\"Model with l2 regularization:\")\n",
        "print(\"Test RMSE = %.4f\" % np.sqrt(mean_squared_error(y_test, y_pred)))\n",
        "print(\"Test R^2 = %.4f\\n\" % r2_score(y_test, y_pred))\n",
        "\n",
        "# Model with l1 regularization.\n",
        "lasso_regression = sm.OLS(y_train, X_train_with_const)\n",
        "lasso_results = lasso_regression.fit_regularized(alpha=0.01, L1_wt=1, refit=True)\n",
        "\n",
        "y_pred = lasso_results.predict(X_test_with_const)\n",
        "print(\"Model with l1 regularization:\")\n",
        "print(\"Test RMSE = %.4f\" % np.sqrt(mean_squared_error(y_test, y_pred)))\n",
        "print(\"Test R^2 = %.4f\\n\" % r2_score(y_test, y_pred))\n",
        "\n",
        "# Model with ElasticNet reqularization.\n",
        "elastic_regression = sm.OLS(y_train, X_train_with_const)\n",
        "elastic_results = elastic_regression.fit_regularized(method='elastic_net', alpha=0.01, L1_wt=0.6, refit=True)\n",
        "print(\"Model with ElasticNet reqularization:\")\n",
        "print(\"Test RMSE = %.4f\" % np.sqrt(mean_squared_error(y_test, y_pred)))\n",
        "print(\"Test R^2 = %.4f\\n\" % r2_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A5ShpmWAevp"
      },
      "source": [
        "#### 3. [1 point] Explore the values of the parameters of the resulting models and compare the number of zero weights in them. Comment on the significance of the coefficients, overal model significance and other related factors from the results table\n",
        "\n",
        "Изучите значения параметров результирующих моделей и сравните количество нулевых весов в них. Прокомментируйте значимость коэффициентов, общую значимость модели и другие связанные факторы из таблицы результатов"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Simple Linear Regression:\\n\", simple_linear_results.summary2())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNhq43BUctCu",
        "outputId": "bc19cefc-2569-4661-ef08-34517bf3607d"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simple Linear Regression:\n",
            "                   Results: Ordinary least squares\n",
            "====================================================================\n",
            "Model:              OLS              Adj. R-squared:     0.907      \n",
            "Dependent Variable: price            AIC:                735145.5726\n",
            "Date:               2022-10-16 20:24 BIC:                735232.2974\n",
            "No. Observations:   43152            Log-Likelihood:     -3.6756e+05\n",
            "Df Model:           9                F-statistic:        4.703e+04  \n",
            "Df Residuals:       43142            Prob (F-statistic): 0.00       \n",
            "R-squared:          0.908            Scale:              1.4660e+06 \n",
            "---------------------------------------------------------------------\n",
            "          Coef.     Std.Err.     t      P>|t|     [0.025      0.975] \n",
            "---------------------------------------------------------------------\n",
            "const    3928.6813    5.8287  674.0213  0.0000   3917.2569  3940.1057\n",
            "x1       5140.0874   27.9240  184.0743  0.0000   5085.3558  5194.8189\n",
            "x2        130.7076    7.0974   18.4163  0.0000    116.7966   144.6186\n",
            "x3        551.4689    6.1732   89.3324  0.0000    539.3693   563.5685\n",
            "x4        825.7541    6.4542  127.9406  0.0000    813.1038   838.4045\n",
            "x5       -120.7683    7.5572  -15.9805  0.0000   -135.5806  -105.9560\n",
            "x6        -65.7545    7.3322   -8.9679  0.0000    -80.1257   -51.3833\n",
            "x7      -1040.8217   43.5594  -23.8943  0.0000  -1126.1991  -955.4444\n",
            "x8         42.9947   26.5090    1.6219  0.1048     -8.9635    94.9529\n",
            "x9        -13.2253   26.6272   -0.4967  0.6194    -65.4151    38.9645\n",
            "--------------------------------------------------------------------\n",
            "Omnibus:             9396.779      Durbin-Watson:         2.004     \n",
            "Prob(Omnibus):       0.000         Jarque-Bera (JB):      431534.424\n",
            "Skew:                -0.050        Prob(JB):              0.000     \n",
            "Kurtosis:            18.492        Condition No.:         18        \n",
            "====================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Метрики из предыдущего пункта:\n",
        "\n",
        "Test RMSE = 1240.1860; Test R^2 = 0.9050. Коэффициент детерминации R^2 = 0.905 высок, близок к единице, что говорит о высоком качестве построенной модели, то есть о значительном соответствии данным. RMSE = 1240.1860 не так велик относительно величины самих данных.\n",
        "\n",
        "Метрики и выводы из таблицы:\n",
        "\n",
        "- Судя по P-value, переменные x8 и x9 незначимы;\n",
        "- Durbin-Watson = 2.004 => автокорреляция отсутствует;\n",
        "- Prob (F-statistic) = 0 => модель адекватна, гипотезе о равенстве всех оцененых коэффициентов нулю не подтвердится;\n",
        "- Omnibus = 9396.779, Prob(Omnibus) = 0.000, вероятность нормального распределения остатков нулевая;"
      ],
      "metadata": {
        "id": "txi_NbaDcvXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Linear Regression with L1 regularization:\\n\", lasso_results.summary2())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSBS9y0Wcv1l",
        "outputId": "2771938d-8072-4400-a237-3edcdf39380d"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression with L1 regularization:\n",
            "                   Results: Ordinary least squares\n",
            "====================================================================\n",
            "Model:              OLS              Adj. R-squared:     0.907      \n",
            "Dependent Variable: price            AIC:                735147.5726\n",
            "Date:               2022-10-16 20:24 BIC:                735242.9699\n",
            "No. Observations:   43152            Log-Likelihood:     -3.6756e+05\n",
            "Df Model:           10               F-statistic:        4.233e+04  \n",
            "Df Residuals:       43142            Prob (F-statistic): 0.00       \n",
            "Method:             elastic_net      Scale:              1.4660e+06 \n",
            "R-squared:          0.908                                           \n",
            "---------------------------------------------------------------------\n",
            "          Coef.     Std.Err.     t      P>|t|     [0.025      0.975] \n",
            "---------------------------------------------------------------------\n",
            "const    3928.6813    5.8287  674.0213  0.0000   3917.2569  3940.1057\n",
            "x1       5140.0874   27.9240  184.0743  0.0000   5085.3558  5194.8189\n",
            "x2        130.7076    7.0974   18.4163  0.0000    116.7966   144.6186\n",
            "x3        551.4689    6.1732   89.3324  0.0000    539.3693   563.5685\n",
            "x4        825.7541    6.4542  127.9406  0.0000    813.1038   838.4045\n",
            "x5       -120.7683    7.5572  -15.9805  0.0000   -135.5806  -105.9560\n",
            "x6        -65.7545    7.3322   -8.9679  0.0000    -80.1257   -51.3833\n",
            "x7      -1040.8217   43.5594  -23.8943  0.0000  -1126.1991  -955.4444\n",
            "x8         42.9947   26.5090    1.6219  0.1048     -8.9635    94.9529\n",
            "x9        -13.2253   26.6272   -0.4967  0.6194    -65.4151    38.9645\n",
            "--------------------------------------------------------------------\n",
            "Omnibus:             9396.779      Durbin-Watson:         2.004     \n",
            "Prob(Omnibus):       0.000         Jarque-Bera (JB):      431534.424\n",
            "Skew:                -0.050        Prob(JB):              0.000     \n",
            "Kurtosis:            18.492        Condition No.:         18        \n",
            "====================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Метрики из предыдущего пункта:\n",
        "Test RMSE = 1250.8344; Test R^2 = 0.9034 Коэффициент детерминации R^2 высок, близок к единице, что говорит о высоком качестве построенной модели, то есть о значительном соответствии данным. RMSE не так велик относительно величины самих данных.\n",
        "\n",
        "Метрики и выводы из таблицы:\n",
        "- Судя по P-value, переменные x8 и x9 незначимы;\n",
        "- Durbin-Watson = 2.004 => автокорреляция отсутствует;\n",
        "- Prob (F-statistic) = 0 => модель адекватна, гипотезе о равенстве всех оцененых коэффициентов нулю не подтвердится;\n",
        "- Omnibus = 9396.779, Prob(Omnibus) = 0.000, вероятность нормального распределения остатков нулевая;"
      ],
      "metadata": {
        "id": "uUYwjt50czgx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tools.tools import pinv_extended\n",
        "pinv_wexog,_ = pinv_extended(ridge_regression.wexog)\n",
        "normalized_cov_params = np.dot(pinv_wexog, np.transpose(pinv_wexog))\n",
        "print(\"\\nLinear Regression with L2 reqularization:\\n\", sm.regression.linear_model.OLSResults(ridge_regression,ridge_results.params,normalized_cov_params).summary2())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3L4UpSlgX6pS",
        "outputId": "cdea3c14-46ce-4fc8-dfca-c32981356698"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Linear Regression with L2 reqularization:\n",
            "                   Results: Ordinary least squares\n",
            "====================================================================\n",
            "Model:              OLS              Adj. R-squared:     0.905      \n",
            "Dependent Variable: price            AIC:                736400.9569\n",
            "Date:               2022-10-16 20:24 BIC:                736487.6818\n",
            "No. Observations:   43152            Log-Likelihood:     -3.6819e+05\n",
            "Df Model:           9                F-statistic:        4.554e+04  \n",
            "Df Residuals:       43142            Prob (F-statistic): 0.00       \n",
            "R-squared:          0.905            Scale:              1.5093e+06 \n",
            "---------------------------------------------------------------------\n",
            "            Coef.    Std.Err.     t      P>|t|     [0.025     0.975] \n",
            "---------------------------------------------------------------------\n",
            "const     3889.7835    5.9141  657.7108  0.0000  3878.1917  3901.3753\n",
            "x1        4163.1002   28.3331  146.9339  0.0000  4107.5667  4218.6337\n",
            "x2         133.7020    7.2014   18.5662  0.0000   119.5872   147.8169\n",
            "x3         520.9041    6.2637   83.1627  0.0000   508.6272   533.1810\n",
            "x4         828.7971    6.5488  126.5577  0.0000   815.9614   841.6328\n",
            "x5         -65.9100    7.6680   -8.5955  0.0000   -80.9393   -50.8806\n",
            "x6         -55.5137    7.4396   -7.4619  0.0000   -70.0955   -40.9319\n",
            "x7         -96.9479   44.1977   -2.1935  0.0283  -183.5762   -10.3195\n",
            "x8          52.3637   26.8975    1.9468  0.0516    -0.3559   105.0832\n",
            "x9         -19.2516   27.0173   -0.7126  0.4761   -72.2060    33.7029\n",
            "--------------------------------------------------------------------\n",
            "Omnibus:             9539.038      Durbin-Watson:         2.002     \n",
            "Prob(Omnibus):       0.000         Jarque-Bera (JB):      175255.844\n",
            "Skew:                0.585         Prob(JB):              0.000     \n",
            "Kurtosis:            12.803        Condition No.:         18        \n",
            "====================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Метрики из предыдущего пункта:\n",
        "\n",
        "Test RMSE = 1240.1860; Test R^2 = 0.9050. Коэффициент детерминации R^2 = 0.905 высок, близок к единице, что говорит о высоком качестве построенной модели, то есть о значительном соответствии данным. RMSE = 1240.1860 не так велик относительно величины самих данных.\n",
        "\n",
        "Метрики и выводы из таблицы:\n",
        "\n",
        "- Судя по P-value, переменные x8 и x9 незначимы;\n",
        "- Durbin-Watson = 2.002 => автокорреляция отсутствует;\n",
        "- Prob (F-statistic) = 0 => модель адекватна, гипотезе о равенстве всех оцененых коэффициентов нулю не подтвердится;\n",
        "- Omnibus = 9539.038, Prob(Omnibus) = 0.000, вероятность нормального распределения остатков нулевая;"
      ],
      "metadata": {
        "id": "iNqDG9Dzc4kT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nLinear Regression with ElasticNet regularization:\\n\", elastic_results.summary2())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BDYJsgGc5kO",
        "outputId": "734927df-076b-46a7-fbe4-7232f49a3dde"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Linear Regression with ElasticNet regularization:\n",
            "                   Results: Ordinary least squares\n",
            "====================================================================\n",
            "Model:              OLS              Adj. R-squared:     0.907      \n",
            "Dependent Variable: price            AIC:                735147.5726\n",
            "Date:               2022-10-16 20:24 BIC:                735242.9699\n",
            "No. Observations:   43152            Log-Likelihood:     -3.6756e+05\n",
            "Df Model:           10               F-statistic:        4.233e+04  \n",
            "Df Residuals:       43142            Prob (F-statistic): 0.00       \n",
            "Method:             elastic_net      Scale:              1.4660e+06 \n",
            "R-squared:          0.908                                           \n",
            "---------------------------------------------------------------------\n",
            "          Coef.     Std.Err.     t      P>|t|     [0.025      0.975] \n",
            "---------------------------------------------------------------------\n",
            "const    3928.6813    5.8287  674.0213  0.0000   3917.2569  3940.1057\n",
            "x1       5140.0874   27.9240  184.0743  0.0000   5085.3558  5194.8189\n",
            "x2        130.7076    7.0974   18.4163  0.0000    116.7966   144.6186\n",
            "x3        551.4689    6.1732   89.3324  0.0000    539.3693   563.5685\n",
            "x4        825.7541    6.4542  127.9406  0.0000    813.1038   838.4045\n",
            "x5       -120.7683    7.5572  -15.9805  0.0000   -135.5806  -105.9560\n",
            "x6        -65.7545    7.3322   -8.9679  0.0000    -80.1257   -51.3833\n",
            "x7      -1040.8217   43.5594  -23.8943  0.0000  -1126.1991  -955.4444\n",
            "x8         42.9947   26.5090    1.6219  0.1048     -8.9635    94.9529\n",
            "x9        -13.2253   26.6272   -0.4967  0.6194    -65.4151    38.9645\n",
            "--------------------------------------------------------------------\n",
            "Omnibus:             9396.779      Durbin-Watson:         2.004     \n",
            "Prob(Omnibus):       0.000         Jarque-Bera (JB):      431534.424\n",
            "Skew:                -0.050        Prob(JB):              0.000     \n",
            "Kurtosis:            18.492        Condition No.:         18        \n",
            "====================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Метрики из предыдущего пункта:\n",
        "\n",
        "Test RMSE = 1240.1860; Test R^2 = 0.9050. Коэффициент детерминации R^2 = 0.905 высок, близок к единице, что говорит о высоком качестве построенной модели, то есть о значительном соответствии данным. RMSE = 1240.1860 не так велик относительно величины самих данных.\n",
        "\n",
        "Метрики и выводы из таблицы:\n",
        "\n",
        "- Судя по P-value, переменные x8 и x9 незначимы;\n",
        "- Durbin-Watson = 2.004 => автокорреляция отсутствует;\n",
        "- Prob (F-statistic) = 0 => модель адекватна, гипотезе о равенстве всех оцененых коэффициентов нулю не подтвердится;\n",
        "- Omnibus = 9396.779, Prob(Omnibus) = 0.000, вероятность нормального распределения остатков нулевая;"
      ],
      "metadata": {
        "id": "ReCN0zj1c55f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сравним полученные модели по оставшимся метрикам.\n",
        "\n",
        "AIC BIC - Меньший AIC и BIC у первой модели, это позволяет только сказать, что в сравнении с остальными тремя она хуже переобучена, то есть лучше.\n",
        "\n",
        "По R^2 и RMSE можно сказать, что модели схожи, разве что вторая показывает себя чуть хуже.\n",
        "\n",
        "несмещённому R^2 в таблицах от statmodels, то чуть хуже по качеству себя показывает третья модель."
      ],
      "metadata": {
        "id": "QB4rTt_pcFPf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLN0ir5kAevq"
      },
      "source": [
        "#### 4. [1 point] Implement one of the elimination algorithms that were described in the Seminar_4 (Elimination by P-value, Forward elimination, Backward elimination), make conclusions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvZPA4NhAevr",
        "outputId": "469b3349-21e9-418c-d8f1-9ff560a13333"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8491996358008097\n",
            "0.8857580227879949\n",
            "0.9026002165823088\n",
            "0.9047856615075786\n",
            "0.9067972180905649\n",
            "0.9073073202231309\n",
            "0.9074804102461187\n",
            "0.9074835724317534\n"
          ]
        }
      ],
      "source": [
        "# Forward elimination by adjusted R squared.\n",
        "variables = {}\n",
        "for i in range(1, X_train_with_const.shape[1]):\n",
        "  variables[i] = X_train_with_const[:,i]\n",
        "\n",
        "# Constant from the beginning\n",
        "base_X = X_train_with_const[:,0]\n",
        "\n",
        "best_r_squared_adjusted = -1\n",
        "best_variable_index = -1\n",
        "\n",
        "while True:\n",
        "  best_variable_index = -1\n",
        "  for key in variables:\n",
        "    temp_X = base_X.copy()\n",
        "    temp_X = np.column_stack((temp_X, variables[key]))\n",
        "    model = sm.OLS(y_train, temp_X).fit()\n",
        "\n",
        "    #if(round(model.rsquared_adj,5) > best_r_squared_adjusted):\n",
        "    if(model.rsquared_adj > best_r_squared_adjusted):\n",
        "      best_r_squared_adjusted = model.rsquared_adj\n",
        "      best_variable_index = key\n",
        "  if best_variable_index == -1:\n",
        "    break\n",
        "  else:\n",
        "    base_X = np.column_stack((base_X, variables[best_variable_index]))\n",
        "    print(best_r_squared_adjusted)\n",
        "    variables.pop(best_variable_index)\n",
        "\n",
        "# base_X contains all variables which upgrade adjusted R squared metric.\n",
        "# Every iteration code prints the best adjusted R squared."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Изначально в итеративном наборе храним только единицы для констант. Потом для каждой переменной проверяем: увеличится ли несмещённый коэффициент детерминации если добавить этот признак к модели. Если какой-то признак увеличил adjusted R^2 запоминаем его клюс в словаре, если он дал максимальный прирост окончательно добавляем его в набор и удаляем из словаря для перебора.\n",
        "\n",
        "Программа вывела максимальные adjusted R^2 на каждой итерации, видно, что числа идут в порядке возрастания, как и положено. Заметим, что один признак не попал в итоговый набор, а последний признак набор увеличил метрику лишь на 0.0000035724317534, вполне возможно, что этот и пропущеный признаки и есть те самые два признака, которые оказались незначимы, как это было описано выше. Поэтому в коде оставляю закомментированную строку, которая при сравнении округляет число до 5 знаков после запятой, тогда последний признак не попадёт в лучший набор признаков."
      ],
      "metadata": {
        "id": "UZJeBlV1DlUP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xS34IltPAevr"
      },
      "source": [
        "#### 5. [1 point] Find the best (in terms of RMSE) $\\alpha$ for Lasso regression using cross-validation with 4 folds. You must select values from range $[10^{-4}, 10^{3}]$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0tr8cfnAevr",
        "outputId": "b5d3ff18-904a-463a-d747-ca8a83f9d25a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'alpha': 1.9306977288832496}\n"
          ]
        }
      ],
      "source": [
        "# Lasso is a sklearn.linear_model\n",
        "lasso_grid_search = GridSearchCV(Lasso(), {'alpha':np.logspace(-4, 3, num = 50, base = 10)}, scoring='neg_root_mean_squared_error', cv=4)\n",
        "lasso_grid_search.fit(X_train_with_const, y_train)\n",
        "print(lasso_grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZlYfd9BAevs"
      },
      "source": [
        "## Gradient descent\n",
        "\n",
        "#### 6. [3.5 points] Implement a Ridge regression model for the MSE loss function, trained by gradient descent.\n",
        "\n",
        "All calculations must be vectorized, and python loops can only be used for gradient descent iterations. As a stop criterion, you must use (simultaneously):\n",
        "\n",
        "* checking for the Absolute-value norm of the weight difference on two adjacent iterations (for example, less than some small number of the order of $10^{-6}$, set by the `tolerance` parameter);\n",
        "* reaching the maximum number of iterations (for example, 10000, set by the `max_iter` parameter).\n",
        "\n",
        "You need to implement:\n",
        "\n",
        "* Full gradient descent:\n",
        "\n",
        "$$\n",
        "w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} Q(w_{k}).\n",
        "$$\n",
        "\n",
        "* Stochastic Gradient Descent:\n",
        "\n",
        "$$\n",
        "w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} q_{i_{k}}(w_{k}).\n",
        "$$\n",
        "\n",
        "$\\nabla_{w} q_{i_{k}}(w_{k}) \\, $ is the estimate of the gradient over the batch of objects selected randomly.\n",
        "\n",
        "* Momentum method:\n",
        "\n",
        "$$\n",
        "h_0 = 0, \\\\\n",
        "h_{k + 1} = \\alpha h_{k} + \\eta_k \\nabla_{w} Q(w_{k}), \\\\\n",
        "w_{k + 1} = w_{k} - h_{k + 1}.\n",
        "$$\n",
        "\n",
        "* Adagrad method:\n",
        "\n",
        "$$\n",
        "G_0 = 0, \\\\\n",
        "G_{k + 1} = G_{k} + (\\nabla_{w} Q(w_{k+1}))^2, \\\\\n",
        "w_{k + 1} = w_{k} - \\eta * \\frac{\\nabla_{w} Q(w_{k+1})}{\\sqrt{G_{k+1} + \\epsilon}}.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "To make sure that the optimization process really converges, we will use the `loss_history` class attribute. After calling the `fit` method, it should contain the values of the loss function for all iterations, starting from the first one (before the first step on the anti-gradient).\n",
        "\n",
        "You need to initialize the weights with a random vector from normal distribution. The following is a template class that needs to contain the code implementing all variations of the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "e-E5d_5zAevt",
        "outputId": "f6916a41-51cb-4c29-fbfc-f6e7ba5e9c13"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m68\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ],
      "source": [
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "class LinReg(BaseEstimator):\n",
        "    def __init__(self, delta=1.0, gd_type='Momentum', \n",
        "                 tolerance=1e-4, max_iter=1000, w0=None, eta=1e-2, alpha=1e-3):\n",
        "        \"\"\"\n",
        "        gd_type: str\n",
        "            'GradientDescent', 'StochasticDescent', 'Momentum', 'Adagrad'\n",
        "        delta: float\n",
        "            proportion of object in a batch (for stochastic GD)\n",
        "        tolerance: float\n",
        "            for stopping gradient descent\n",
        "        max_iter: int\n",
        "            maximum number of steps in gradient descent\n",
        "        w0: np.array of shape (d)\n",
        "            init weights\n",
        "        eta: float\n",
        "            learning rate\n",
        "        alpha: float\n",
        "            momentum coefficient\n",
        "        reg_cf: float\n",
        "            regularization coefficient\n",
        "        epsilon: float\n",
        "            numerical stability\n",
        "        \"\"\"\n",
        "        \n",
        "        self.delta = delta\n",
        "        self.gd_type = gd_type\n",
        "        self.tolerance = tolerance\n",
        "        self.max_iter = max_iter\n",
        "        self.w0 = w0\n",
        "        self.alpha = alpha\n",
        "        self.w = None\n",
        "        self.eta = eta\n",
        "        self.loss_history = None # list of loss function values at each training iteration\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        X: np.array of shape (l, d)\n",
        "        y: np.array of shape (l)\n",
        "        ---\n",
        "        output: self\n",
        "        \"\"\"\n",
        "        self.loss_history = []\n",
        "        n, k = X.shape\n",
        "        if self.w0 is None:\n",
        "          self.w = np.random.normal(0, 1, k+1)\n",
        "        else:\n",
        "          self.w = self.w0\n",
        "        return self\n",
        "    \n",
        "    def predict(self, X):\n",
        "        if self.w is None:\n",
        "            raise Exception('Not trained yet')\n",
        "        return np.dot(X, self.w)\n",
        "    \n",
        "    def calc_gradient(self, X, y):\n",
        "      match(self.gd_type):\n",
        "          case 'GradientDescent':\n",
        "              print('Hello to you too!')\n",
        "          case 'StochasticDescent':\n",
        "              print('Hello to you too!')\n",
        "          case 'Momentum':\n",
        "              print('Hello to you too!')\n",
        "          case 'Adagrad':\n",
        "              print('Hello to you too!')\n",
        "\n",
        "        \"\"\"\n",
        "        X: np.array of shape (l, d) (l can be equal to 1 if stochastic)\n",
        "        y: np.array of shape (l)\n",
        "        ---\n",
        "        output: np.array of shape (d)\n",
        "        \"\"\"\n",
        "\n",
        "    def calc_loss(self, X, y):\n",
        "        error = y-self.predict(X)\n",
        "        return 1.0/(y.size)*np.dot(error.T, error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXddrgHHAevt"
      },
      "source": [
        "#### 7. [1 points] Train and validate \"hand-written\" models on the same data, and compare the quality with the Sklearn or StatsModels methods. Investigate the effect of the `max_iter` and `alpha` parameters on the optimization process. Is it consistent with your expectations?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fv_XxE9jAevu"
      },
      "outputs": [],
      "source": [
        "# your code here \n",
        "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXqwN4zZAevu"
      },
      "source": [
        "#### 8. [1 points] Plot graphs (on the same picture) of the dependence of the loss function value on the iteration number for Full GD, SGD, Momentum and Adagrad. Draw conclusions about the rate of convergence of various modifications of gradient descent.\n",
        "\n",
        "Don't forget about what *beautiful* graphics should look like!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qdtdk5NLAevu"
      },
      "outputs": [],
      "source": [
        "# your code here \n",
        "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pap11RyTAevv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}